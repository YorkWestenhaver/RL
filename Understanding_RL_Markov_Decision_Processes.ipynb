{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Understanding RL - Markov Decision Processes.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wI2D7QYw5aZ9",
        "92iLbtUy_weT",
        "xwybKGKpffiz",
        "edfFK_nplee4",
        "PWIdQRrUpjSR"
      ],
      "authorship_tag": "ABX9TyMjJrxFsJ8D7rYrSJRYYRxK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YorkWestenhaver/RL/blob/main/Understanding_RL_Markov_Decision_Processes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG8CLFp54qYI"
      },
      "source": [
        "# Markov Decision Processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPdhKrwd_bgC"
      },
      "source": [
        "## Dynamic Programming:\n",
        "\n",
        "Simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI2D7QYw5aZ9"
      },
      "source": [
        "### 1. Objective Functions\n",
        "\n",
        "Any optimization problem has some objective: minimizing travel time, minimizing cost, maximizing profits, maximizing utility, etc. The mathematical function that describes this objective is called the objective function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ3eYAgf-_oz"
      },
      "source": [
        "**Objective Functions** take TWO forms:\n",
        "\n",
        "1. <font color=red>**The Loss Function form:**</font>\n",
        "> A **Loss Function** aims to maximize the \"goodness\" of an action by minimizing the \"*cost*\" of that action; hence, loss functions are also sometimes called *Cost Functions*.\n",
        "2. <font color=green>**The Reward Function form:**</font>\n",
        ">A **Reward Runction** is (in some sense) the negative of a Loss Function; it aims to maximize the \"goodness\" of an action by maximizing the \"*reward*\" of that action; hence, reward functions are also called *profit functions*, *utility functions*, *fitness functions*, etc.—depending on the situation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92iLbtUy_weT"
      },
      "source": [
        "### 2. The Bellman Equations\n",
        "\n",
        "Breaking a large problem into a sequence of sub-problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ8wd0phDIkC"
      },
      "source": [
        "Optimization Problems that *span several decrete points in* <font color=limegreen>***time***</font> can often be broken down into smaller **sub-problems**. If a <font size=5px>***larger problem***</font> can be solved optimally by <font color=redorange>**breaking**</font> it into <font size=2px>***sub-problems***</font>, then by finding the optimal solutions to those <font size=2px>***sub-problems***</font>, then  the larger problem is said to have *optimal substructure*.\n",
        ">think of breaking a minute-long task into 1-second subtasks; then optimally completing those 1-second subtasks.\n",
        "\n",
        "This relationship (between the <font color=orange>*value*</font> of the <font size=5px>***larger problem***</font> and the <font color=orange>*values*</font> of the <font size=2px>***sub-problems***</font>) is characterized by the **Bellman equations**.\n",
        "\n",
        ">Keep in mind, however, that some Optimization Problems *cannot* be taken apart this way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5CfUAWHGGrK"
      },
      "source": [
        "More Formally, the Bellman Equations show that a dynamic optimization problem in discrete time can be stated in a recursive, step-by-step form by writing down the relationship between the value function in one period and the value function in the next period.\n",
        "\n",
        "This process of reasoning backwards in time, from the end to beginning of a problem, to determine the optimal sequence of actions is known as ***backward induction***.\n",
        "\n",
        "---\n",
        "\n",
        "Assuming the following:\n",
        "- $\\color{orange}{S}_{\\color{green}{t}}$ = the state at time $\\color{green}{t}$\n",
        "- $\\color{magenta}{V}_{\\color{blue}{\\pi}}^*(\\color{orange}{S}_{\\color{green}{t}})$: the *value* function **$\\color{magenta}{V}$** with the policy **$\\color{blue}{\\pi}$** of the state **$\\color{orange}{S}$** at time **$\\color{green}{t}$**\n",
        ">$\\color{magenta}{V}_{\\color{blue}{\\pi}}^*\\overset{\\Delta}{=}$ the optimal *value* function **$\\color{magenta}{V}$** with the policy **$\\color{blue}{\\pi}$**\n",
        "- $\\color{lime}{a}_\\color{green}{t}$ = the action taken at time $\\color{green}{t}$\n",
        ">At any state-time, the set of possible actions depends on the current state; we can write this as $\\color{lime}{a}_\\color{green}{t} \\in \\Gamma(\\color{orange}{S}_{\\color{green}{t}})$.\n",
        ">>Note that each $\\color{lime}{a}_\\color{green}{t}$ can represent multiple control variables.\n",
        "- $\\color{orange}{S}_{\\color{green}{t}+1}$ = $T(\\color{orange}{S}_{\\color{green}{t}}, \\color{lime}{a}_\\color{green}{t})$\n",
        ">The state changes from $\\color{orange}{S}_{\\color{green}{t}}$ to $\\color{orange}{S}_{\\color{green}{t}+1}$ as described by the transition function $T$, when action $\\color{lime}{a}_\\color{green}{t}$ is taken.\n",
        "- $\\color{red}{\\gamma^{\\color{green}{t}}}$ : the discount factor $\\color{red}{\\gamma}$ — adjusted by the remaining time $\\color{green}{t}$ until the infinite horizon $\\infty$.\n",
        ">$\\color{red}{\\gamma} \\in (0,1]$: the discount factor must be between 0 and 1.\n",
        "\n",
        "\n",
        "The Bellman Equation is thus written:\n",
        "\n",
        "$$\\color{magenta}{V}_{\\color{blue}{\\pi}}(\\color{orange}{S}_{\\color{green}{t}})\\overset{\\Delta}{=}\\text{max}_{\\{\\color{lime}{a}_\\color{green}{t}\\}_{\\color{green}{t}=0}^\\infty}\\sum_{\\color{green}{t}=0}^{\\infty}\\color{red}{\\gamma^{\\color{green}{t}}}\\color{teal}{R}_{\\color{green}{t}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPsjS3ZAevKv"
      },
      "source": [
        "##Part 1: Markov Decision Processes\n",
        " What are they?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwybKGKpffiz"
      },
      "source": [
        "### 1. Understanding  Markov Decision Processes (MDPs):\n",
        "\n",
        "**Markov Decision Processes** are mathematical models that are used to formulate optimization policies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PTY8lk05QFb"
      },
      "source": [
        "The process of ***managing a stock portfolio*** can be described as a *Markov Decision Process*. \n",
        "\n",
        "A **MDP** consists of a tuple ($\\color{orange}{S}_{\\color{green}{t}}$, $\\color{lime}{a}_\\color{green}{t}$,  $\\color{pink}{P}_\\color{green}{t}$, $\\color{teal}{R}_\\color{green}{t}$), where:\n",
        "\n",
        "- $\\color{orange}{S}_{\\color{green}{t}}$ = the state at time $\\color{green}{t}$\n",
        ">$\\color{orange}{S}_{\\color{green}{t}}\\overset{\\Delta}{=}$ the sum price of the assets invested\n",
        "- $\\color{lime}{a}_\\color{green}{t}$ = the action taken at time $\\color{green}{t}$\n",
        ">$\\color{lime}{a}_\\color{green}{t} \\overset{\\Delta}{=} \\color{violet}{w}_{\\color{green}{t}+1}-\\color{violet}{w}_{\\color{green}{t}}$\n",
        ">>where $\\color{violet}{w}_\\color{green}{t}$ and $\\color{violet}{w}_{\\color{green}{t}+1}$ are weight vectors at time $\\color{green}{t}$ and $\\color{green}{t}+1$, respectively.\n",
        "- $\\color{pink}{P}_\\color{green}{t}$ = the probability of transforming the state from $\\color{orange}{S}_{\\color{green}{t}}$ to $\\color{orange}{S}_{\\color{green}{t}+1}$\n",
        "- $\\color{teal}{R}_\\color{green}{t}$ = the reward at time $\\color{green}{t}$\n",
        ">$\\color{teal}{R}_\\color{green}{t}\\overset{\\Delta}{=} \\color{cyan}{p}_{\\color{green}{t}+1}-\\color{cyan}{p}_{\\color{green}{t}}$\n",
        ">>where $\\color{cyan}{p}_{\\color{green}{t}+1}$ and $\\color{cyan}{p}_{\\color{green}{t}}$ are the portfolios at time $\\color{green}{t}$ and $\\color{green}{t}+1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BMbVc8S3-bs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edfFK_nplee4"
      },
      "source": [
        "### 2. The simple Value Function: $\\color{magenta}{G}$\n",
        "\n",
        "The **simple *Value* function** is defined as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aL2Yz3oyEJZ"
      },
      "source": [
        "$\\color{magenta}{G}_{\\color{blue}{\\pi}}(\\color{orange}{S}_{\\color{green}{t}})\\overset{\\Delta}{=}\\sum_{\\color{purple}{k}=\\color{green}{t}}^{\\color{green}{T}}\\color{red}{\\gamma^{\\color{purple}{k}\\color{black}{-}\\color{green}{t}}}\\color{teal}{R}_{\\color{purple}{k}}$ :\n",
        "\n",
        "- $\\color{magenta}{G}_{\\color{blue}{\\pi}}(\\color{orange}{S}_{\\color{green}{t}})$: the *value* function **$\\color{magenta}{G}$** with the policy **$\\color{blue}{\\pi}$** of the state **$\\color{orange}{S}$** at time **$\\color{green}{t}$**\n",
        "- $\\overset{\\Delta}{=}$ : is defined as\n",
        "- $\\sum_{\\color{purple}{k}=\\color{green}{t}}^{\\color{green}{T}}$ : the sum from the current time $\\color{green}{t}$ to farthest future time (last trading time) $\\color{green}{T}$\n",
        ">this is because the the value & reward of a state depend on the total future values & rewards (of the following time periods).\n",
        "\n",
        "- $\\color{red}{\\gamma^{\\color{purple}{k}\\color{black}{-}\\color{green}{t}}}$ : the discount factor $\\color{red}{\\gamma}$ — adjusted by the remaining time ($\\color{purple}{k}\\color{black}{-}\\color{green}{t}$) until the farthest future time (last trading time), $\\color{green}{T}$.\n",
        ">$\\color{red}{\\gamma} \\in (0,1]$: the discount factor must be between 0 and 1—the discount factor is just a form of exponential decay, in which $\\gamma$ must always be between 0 and 1.\n",
        "\n",
        "- $\\color{teal}{R}_{\\color{purple}{k}}$ : the reward function at time $\\color{purple}{k}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WILjKFk60FBN"
      },
      "source": [
        "Or, in one semi-confusing paragraph summary:\n",
        "\n",
        "The ***Value function*** **$\\color{magenta}{G}$** with the policy **$\\color{blue}{\\pi}$** of the state **$\\color{orange}{S}$** at time **$\\color{green}{t}$** is defined as the sum—from the current time $\\color{green}{t}$ to farthest future time (last trading time) $\\color{green}{T}$—of the rewards $\\color{teal}{R}$ at times $\\color{purple}{k}$, where each of those rewards ($\\color{teal}{R}$) is discounted by $\\color{red}{\\gamma}$, with $\\color{red}{\\gamma}$ adjusted for each time $\\color{purple}{k}$ by the remaining time ($\\color{purple}{k}\\color{black}{-}\\color{green}{t}$) until the farthest future trading time (last trading time), $\\color{green}{T}$. Thus, the value function essentially gives you the weighted sum of all future rewards, each discounted exponentially by it's time-distance from the present."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWIdQRrUpjSR"
      },
      "source": [
        "### 3. The REAL Value Function\n",
        "Accounting for difficulties..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF6GjhAPp4CY"
      },
      "source": [
        "It would be nice if the aforementioned ***simple* value function** ($\\sum_{\\color{purple}{k}=\\color{green}{t}}^{\\color{green}{T}}\\color{red}{\\gamma^{\\color{purple}{k}\\color{black}{-}\\color{green}{t}}}\\color{teal}{R}_{\\color{purple}{k}}$) worked in practice, BUT it usually *doesn't*; in general, $\\color{magenta}{G}_{\\color{blue}{\\pi}}(\\color{orange}{S}_{\\color{green}{t}})$ cannot simply be obtained this way, so we need to *compute* its <font color=salmon>estimated</font> value by using our expectation of $\\color{magenta}{G}_{\\color{blue}{\\pi}}$.\n",
        "\n",
        "Because the policy $\\color{blue}{\\pi}$ is determined by the actions $\\color{lime}{a}$ we take, we first define the ***actual value function*** $\\color{olive}{Q}_{\\color{blue}{\\pi}}$ as:\n",
        "\n",
        "$$\\color{olive}{Q}_{\\color{blue}{\\pi}}(\\color{orange}{S}_{\\color{green}{t}}, \\color{lime}{a}_\\color{green}{t}) \\overset{\\Delta}{=} E[\\color{magenta}{G}_{\\color{blue}{\\pi}}(\\color{orange}{S}_{\\color{green}{t}})]$$\n",
        "\n",
        "Using this function, we can estimate the value of a state, and the actions that can be taken on that state, which is the basic principle of ***classic* Q-Learning**. Thus, wholy formalized, the equation looks like this:\n",
        "\n",
        "$$\\color{olive}{Q}_{\\color{blue}{\\pi}}(\\color{orange}{S}_{\\color{green}{t}}, \\color{lime}{a}_\\color{green}{t}) \\overset{\\Delta}{=} E[\\sum_{\\color{purple}{k}=\\color{green}{t}}^{\\color{green}{T}}\\color{red}{\\gamma^{\\color{purple}{k}\\color{black}{-}\\color{green}{t}}}\\color{teal}{R}_{\\color{purple}{k}}]$$\n",
        "> Or, in summary, we estimate the value of the possible state-actions we *could* take in the current state $\\color{orange}{S}_{\\color{green}{t}}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhIIzhZ0yqc7"
      },
      "source": [
        ">`Learning` **requires** *all* the possible combinations of states and actions to be explicitly *known*; it is not tractible to solve problems with *infinite state space*, such as portfolio management in stock market, using the simple value equation."
      ]
    }
  ]
}